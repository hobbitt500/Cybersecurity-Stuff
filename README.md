This Bash script uses wget to recursively download websites, including directories, assets, and links. 
Which means that depending on the website's size it can be very fast to find all directories compared to regular bruteforce directory searchers.
It helps cybersecurity professionals and researchers analyze website structures quickly. 
Supports recursion depth, random wait times, SSL handling, and offline viewing. 
‚ö†Ô∏è Use only with permission!

How to Use üöÄ

git clone https://github.com/hobbitt500/Fast-Directory-Searching.git


cd YOUR_REPO  


chmod +x Fastestdirsearch.sh  

2Ô∏è‚É£ Run the Script:

./Fastestdirsearch.sh <target_url> [recursion_depth]

Example:
./Fastestdirsearch.sh http://example.com 3

